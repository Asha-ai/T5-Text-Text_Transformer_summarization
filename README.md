### Go through my blog @ Medium 
https://medium.com/@ashaicy99/text-summation-using-transfer-learning-160fa52e10d6
### Advantages of Text Summarization

#### Works Instantly
Reading the entire article, dissecting it and separating the important ideas from the raw text takes time and effort. Reading an article of 500 words can take at least 15 minutes. Automatic summary software summarize texts of 500-5000 words in a split second. This allows the user to read less data but still receive the most important information and make solid conclusions.

#### Works in Any Language
Text summarization  work in any language – an ability that exceeds the abilities of most humans. Since summarizers work on linguistic models they are able to summarize texts in most languages 

### Improves Productivity
We can summarizes not only documents but also web pages. This highly improves productivity as it speeds up the surfing process.

### Does Not Miss Important Facts
A unique features that some software have, is the ability to declare a word whose sentences that include it will automatically appear at the summary. These critical words are usually words with tactical importance, such as ‘bomb’, ‘explode’, etc.

## Types of Text summarization
Text summarization mainly is of 3 types
### 1)Based on input type
Here we can pass single Doc,Multi Doc
### 2)Based on output type
Extractive, Abstractive
### 3)Based on the purpose
Generic, Domain Specific, Query Based

## T5-Text-Text_Transformer_summarization
The Transformer model in NLP has truly changed the way we work with text data
This is Bart_T5-summarization

Transfer learning is the process of training a model on a large-scale dataset and then using that pretrained model to conduct learning for another downstream task (i.e., target task). Transfer learning was popularized in the field of computer vision thanks to the ImageNet dataset.

## Transformer: 

The Transformer is a deep learning model introduced in 2017, used primarily in the field of natural language processing (NLP). Like recurrent neural networks (RNNs), Transformers are designed to handle sequential data, such as natural language, for tasks such as translation and text summarization. 

T5 is an extremely large new neural network model that is trained on a mixture of unlabeled text (the authors’ huge new C4 collection of English web text) and labeled data from popular natural language processing tasks, then fine-tuned individually for each of the tasks that they authors aim to solve. 

## Requirements to download
!pip install -U transformers

!pip install -U torch

## text_summarization_t5vsSpacy
Consists of meetings and its summaries using Spacy and T5
