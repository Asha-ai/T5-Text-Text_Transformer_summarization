## T5-Text-Text_Transformer_summarization
The Transformer model in NLP has truly changed the way we work with text data
This is Bart_T5-summarization

Transfer learning is the process of training a model on a large-scale dataset and then using that pretrained model to conduct learning for another downstream task (i.e., target task). Transfer learning was popularized in the field of computer vision thanks to the ImageNet dataset.

## Transformer: 

The Transformer is a deep learning model introduced in 2017, used primarily in the field of natural language processing (NLP). Like recurrent neural networks (RNNs), Transformers are designed to handle sequential data, such as natural language, for tasks such as translation and text summarization. 

T5 is an extremely large new neural network model that is trained on a mixture of unlabeled text (the authorsâ€™ huge new C4 collection of English web text) and labeled data from popular natural language processing tasks, then fine-tuned individually for each of the tasks that they authors aim to solve. 

## Requirements to download
!pip install -U transformers

!pip install -U torch

!pip install flask

## Examples Doc
Consists of meetings and its summaries using Spacy and T5
